{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:921\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:813\u001b[0m, in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:1289\u001b[0m, in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# TensorFlow / Keras\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
            "File \u001b[1;32mc:\\Users\\moto1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
            "File \u001b[1;32mc:\\Users\\moto1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1334\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import glob\n",
        "import time\n",
        "import itertools\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Sklearn metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix\n",
        ")\n",
        "\n",
        "# Colab helpers (optional)\n",
        "try:\n",
        "    from google.colab import drive, files\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n",
        "print(f\"Running in Colab: {IN_COLAB}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path setup: try to use local `archive/` folder. If not present (e.g., in Colab),\n",
        "# allow uploading a ZIP with the same structure, or mount Drive.\n",
        "\n",
        "import zipfile\n",
        "\n",
        "DATA_DIR_DEFAULT = \"archive\"\n",
        "DATA_DIR = DATA_DIR_DEFAULT\n",
        "\n",
        "if IN_COLAB and not os.path.isdir(DATA_DIR_DEFAULT):\n",
        "    print(\"`archive/` not found. Options:\\n- Upload a ZIP of the archive folder\\n- Or mount Google Drive and set DATA_DIR accordingly.\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    # If user uploads a zip (e.g., archive.zip), unzip it here\n",
        "    for name in uploaded:\n",
        "        if name.lower().endswith('.zip'):\n",
        "            with zipfile.ZipFile(name, 'r') as zip_ref:\n",
        "                zip_ref.extractall('./')\n",
        "            break\n",
        "    if not os.path.isdir(DATA_DIR_DEFAULT):\n",
        "        print(\"Please set DATA_DIR manually to the extracted folder.\")\n",
        "\n",
        "print(\"DATA_DIR:\", os.path.abspath(DATA_DIR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading utilities\n",
        "\n",
        "@dataclass\n",
        "class ClauseRecord:\n",
        "    text: str\n",
        "    category: str  # derived from filename\n",
        "    label: Optional[str] = None  # if present in CSV\n",
        "\n",
        "class ClauseDatasetLoader:\n",
        "    def __init__(self, data_dir: str, text_column_candidates: Optional[List[str]] = None,\n",
        "                 label_column_candidates: Optional[List[str]] = None):\n",
        "        self.data_dir = data_dir\n",
        "        self.text_column_candidates = text_column_candidates or [\n",
        "            'text', 'clause', 'clause_text', 'content', 'body'\n",
        "        ]\n",
        "        self.label_column_candidates = label_column_candidates or [\n",
        "            'label', 'type', 'clause_type', 'category'\n",
        "        ]\n",
        "\n",
        "    def _infer_text_column(self, df: pd.DataFrame) -> str:\n",
        "        for col in self.text_column_candidates:\n",
        "            if col in df.columns:\n",
        "                return col\n",
        "        # fallback: choose the longest-text column\n",
        "        lengths = {col: df[col].astype(str).str.len().mean() for col in df.columns}\n",
        "        return max(lengths, key=lengths.get)\n",
        "\n",
        "    def _infer_label_column(self, df: pd.DataFrame) -> Optional[str]:\n",
        "        for col in self.label_column_candidates:\n",
        "            if col in df.columns:\n",
        "                return col\n",
        "        return None\n",
        "\n",
        "    def load(self) -> List[ClauseRecord]:\n",
        "        pattern = os.path.join(self.data_dir, '*.csv')\n",
        "        files = sorted(glob.glob(pattern))\n",
        "        records: List[ClauseRecord] = []\n",
        "        for path in files:\n",
        "            category = os.path.splitext(os.path.basename(path))[0]\n",
        "            try:\n",
        "                df = pd.read_csv(path)\n",
        "            except Exception:\n",
        "                try:\n",
        "                    df = pd.read_csv(path, encoding='latin-1')\n",
        "                except Exception as e:\n",
        "                    print(f\"Skipping {path}: {e}\")\n",
        "                    continue\n",
        "            if df.empty:\n",
        "                continue\n",
        "            text_col = self._infer_text_column(df)\n",
        "            label_col = self._infer_label_column(df)\n",
        "            for _, row in df.iterrows():\n",
        "                text = str(row.get(text_col, '')).strip()\n",
        "                if not text:\n",
        "                    continue\n",
        "                label = str(row.get(label_col)) if (label_col is not None and label_col in df.columns) else None\n",
        "                records.append(ClauseRecord(text=text, category=category, label=label))\n",
        "        return records\n",
        "\n",
        "loader = ClauseDatasetLoader(DATA_DIR)\n",
        "clauses = loader.load()\n",
        "print(f\"Loaded clauses: {len(clauses)} across CSVs in {DATA_DIR}\")\n",
        "\n",
        "# Show a small sample\n",
        "for rec in random.sample(clauses, min(3, len(clauses))):\n",
        "    print(f\"[{rec.category}] {rec.text[:120]}{'...' if len(rec.text)>120 else ''}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pair generation: create positive pairs (same category) and negative pairs (different categories)\n",
        "\n",
        "@dataclass\n",
        "class PairRecord:\n",
        "    text_a: str\n",
        "    text_b: str\n",
        "    label: int  # 1 for similar, 0 for not similar\n",
        "    cat_a: str\n",
        "    cat_b: str\n",
        "\n",
        "class PairBuilder:\n",
        "    def __init__(self, clauses: List[ClauseRecord], max_pos_pairs_per_cat: int = 5000,\n",
        "                 max_neg_pairs: int = 200000):\n",
        "        self.clauses = clauses\n",
        "        self.max_pos_pairs_per_cat = max_pos_pairs_per_cat\n",
        "        self.max_neg_pairs = max_neg_pairs\n",
        "\n",
        "    def build(self) -> List[PairRecord]:\n",
        "        by_cat: Dict[str, List[ClauseRecord]] = {}\n",
        "        for rec in self.clauses:\n",
        "            by_cat.setdefault(rec.category, []).append(rec)\n",
        "        categories = list(by_cat.keys())\n",
        "\n",
        "        pairs: List[PairRecord] = []\n",
        "        # Positive pairs\n",
        "        for cat, recs in by_cat.items():\n",
        "            if len(recs) < 2:\n",
        "                continue\n",
        "            indices = list(range(len(recs)))\n",
        "            all_pos = list(itertools.combinations(indices, 2))\n",
        "            random.shuffle(all_pos)\n",
        "            limit = min(len(all_pos), self.max_pos_pairs_per_cat)\n",
        "            for i, j in all_pos[:limit]:\n",
        "                a = recs[i].text\n",
        "                b = recs[j].text\n",
        "                pairs.append(PairRecord(a, b, 1, cat, cat))\n",
        "\n",
        "        # Negative pairs sampled across categories\n",
        "        cat_pairs = list(itertools.combinations(categories, 2))\n",
        "        random.shuffle(cat_pairs)\n",
        "        neg_pairs_added = 0\n",
        "        for (c1, c2) in cat_pairs:\n",
        "            recs1 = by_cat[c1]\n",
        "            recs2 = by_cat[c2]\n",
        "            if not recs1 or not recs2:\n",
        "                continue\n",
        "            # sample min size to limit combinations\n",
        "            sample1 = random.sample(recs1, min(len(recs1), 200))\n",
        "            sample2 = random.sample(recs2, min(len(recs2), 200))\n",
        "            combos = list(itertools.product(sample1, sample2))\n",
        "            random.shuffle(combos)\n",
        "            for r1, r2 in combos:\n",
        "                pairs.append(PairRecord(r1.text, r2.text, 0, r1.category, r2.category))\n",
        "                neg_pairs_added += 1\n",
        "                if neg_pairs_added >= self.max_neg_pairs:\n",
        "                    break\n",
        "            if neg_pairs_added >= self.max_neg_pairs:\n",
        "                break\n",
        "\n",
        "        random.shuffle(pairs)\n",
        "        return pairs\n",
        "\n",
        "pair_builder = PairBuilder(clauses)\n",
        "pairs = pair_builder.build()\n",
        "print(f\"Total pairs: {len(pairs)} (pos/neg split approx unknown until counting)\")\n",
        "print(pd.Series([p.label for p in pairs]).value_counts())\n",
        "\n",
        "# Train/Val/Test split on pairs, stratified by label\n",
        "labels = np.array([p.label for p in pairs])\n",
        "train_idx, temp_idx = train_test_split(np.arange(len(pairs)), test_size=0.3, random_state=SEED, stratify=labels)\n",
        "labels_temp = labels[temp_idx]\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=SEED, stratify=labels_temp)\n",
        "\n",
        "print(f\"Split sizes: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing and vectorization\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class TextVectorizer:\n",
        "    def __init__(self, num_words: int = 50000, oov_token: str = \"<OOV>\", max_len: int = 256):\n",
        "        self.num_words = num_words\n",
        "        self.oov_token = oov_token\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer: Optional[Tokenizer] = None\n",
        "\n",
        "    def fit(self, texts: List[str]):\n",
        "        self.tokenizer = Tokenizer(num_words=self.num_words, oov_token=self.oov_token)\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    def transform(self, texts: List[str]) -> np.ndarray:\n",
        "        assert self.tokenizer is not None\n",
        "        seqs = self.tokenizer.texts_to_sequences(texts)\n",
        "        return pad_sequences(seqs, maxlen=self.max_len, padding='post', truncating='post')\n",
        "\n",
        "    def fit_transform(self, texts: List[str]) -> np.ndarray:\n",
        "        self.fit(texts)\n",
        "        return self.transform(texts)\n",
        "\n",
        "# Build raw text arrays for train/val/test\n",
        "train_pairs = [pairs[i] for i in train_idx]\n",
        "val_pairs = [pairs[i] for i in val_idx]\n",
        "test_pairs = [pairs[i] for i in test_idx]\n",
        "\n",
        "train_texts = [p.text_a for p in train_pairs] + [p.text_b for p in train_pairs]\n",
        "val_texts = [p.text_a for p in val_pairs] + [p.text_b for p in val_pairs]\n",
        "test_texts = [p.text_a for p in test_pairs] + [p.text_b for p in test_pairs]\n",
        "\n",
        "vectorizer = TextVectorizer(num_words=50000, max_len=256)\n",
        "vectorizer.fit(train_texts)\n",
        "\n",
        "# Vectorize pairs\n",
        "X_train_a = vectorizer.transform([p.text_a for p in train_pairs])\n",
        "X_train_b = vectorizer.transform([p.text_b for p in train_pairs])\n",
        "y_train = np.array([p.label for p in train_pairs], dtype=np.int32)\n",
        "\n",
        "X_val_a = vectorizer.transform([p.text_a for p in val_pairs])\n",
        "X_val_b = vectorizer.transform([p.text_b for p in val_pairs])\n",
        "y_val = np.array([p.label for p in val_pairs], dtype=np.int32)\n",
        "\n",
        "X_test_a = vectorizer.transform([p.text_a for p in test_pairs])\n",
        "X_test_b = vectorizer.transform([p.text_b for p in test_pairs])\n",
        "y_test = np.array([p.label for p in test_pairs], dtype=np.int32)\n",
        "\n",
        "vocab_size = min(vectorizer.num_words, 1 + len(vectorizer.tokenizer.word_index) if vectorizer.tokenizer else vectorizer.num_words)\n",
        "max_len = vectorizer.max_len\n",
        "print(f\"Vocab size: {vocab_size}, Max len: {max_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model factory: Siamese encoders and classifiers\n",
        "\n",
        "class AdditiveAttention(layers.Layer):\n",
        "    def __init__(self, units: int, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.W = layers.Dense(units, activation='tanh')\n",
        "        self.v = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # inputs: (batch, seq_len, hidden)\n",
        "        score = self.v(self.W(inputs))  # (batch, seq_len, 1)\n",
        "        weights = tf.nn.softmax(score, axis=1)  # (batch, seq_len, 1)\n",
        "        context = tf.reduce_sum(weights * inputs, axis=1)  # (batch, hidden)\n",
        "        return context\n",
        "\n",
        "    def get_config(self):\n",
        "        cfg = super().get_config()\n",
        "        cfg.update({\"units\": self.units})\n",
        "        return cfg\n",
        "\n",
        "class SiameseEncoderFactory:\n",
        "    @staticmethod\n",
        "    def create_bilstm_attention(vocab_size: int, embed_dim: int = 128, lstm_units: int = 128,\n",
        "                                max_len: int = 256, dropout_rate: float = 0.2) -> keras.Model:\n",
        "        inp = layers.Input(shape=(max_len,), name='input_ids')\n",
        "        x = layers.Embedding(vocab_size, embed_dim, mask_zero=True, name='embedding')(inp)\n",
        "        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        x = AdditiveAttention(units=lstm_units)(x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        out = layers.Dense(128, activation=None, name='sentence_embedding')(x)\n",
        "        return keras.Model(inp, out, name='BiLSTM_Attn_Encoder')\n",
        "\n",
        "    @staticmethod\n",
        "    def create_cnn(vocab_size: int, embed_dim: int = 128, num_filters: int = 128,\n",
        "                   kernel_sizes: Tuple[int, ...] = (3, 4, 5), max_len: int = 256,\n",
        "                   dropout_rate: float = 0.2) -> keras.Model:\n",
        "        inp = layers.Input(shape=(max_len,), name='input_ids')\n",
        "        x = layers.Embedding(vocab_size, embed_dim, mask_zero=False, name='embedding')(inp)\n",
        "        conv_outputs = []\n",
        "        for k in kernel_sizes:\n",
        "            c = layers.Conv1D(num_filters, k, activation='relu', padding='valid')(x)\n",
        "            p = layers.GlobalMaxPooling1D()(c)\n",
        "            conv_outputs.append(p)\n",
        "        h = layers.Concatenate()(conv_outputs) if len(conv_outputs) > 1 else conv_outputs[0]\n",
        "        h = layers.Dropout(dropout_rate)(h)\n",
        "        h = layers.Dense(256, activation='relu')(h)\n",
        "        h = layers.Dropout(dropout_rate)(h)\n",
        "        out = layers.Dense(128, activation=None, name='sentence_embedding')(h)\n",
        "        return keras.Model(inp, out, name='CNN_Encoder')\n",
        "\n",
        "class SiameseClassifier:\n",
        "    @staticmethod\n",
        "    def build_from_encoder(encoder: keras.Model, max_len: int = 256, dropout_rate: float = 0.2) -> keras.Model:\n",
        "        a = layers.Input(shape=(max_len,), name='input_a')\n",
        "        b = layers.Input(shape=(max_len,), name='input_b')\n",
        "        ea = encoder(a)\n",
        "        eb = encoder(b)\n",
        "        # Combine with absolute difference and elementwise product\n",
        "        diff = tf.math.abs(ea - eb)\n",
        "        prod = ea * eb\n",
        "        merged = layers.Concatenate()([ea, eb, diff, prod])\n",
        "        x = layers.LayerNormalization()(merged)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        out = layers.Dense(1, activation='sigmoid')(x)\n",
        "        model = keras.Model(inputs=[a, b], outputs=out)\n",
        "        return model\n",
        "\n",
        "# Instantiate models\n",
        "encoder_bilstm = SiameseEncoderFactory.create_bilstm_attention(vocab_size=vocab_size, max_len=max_len)\n",
        "model_bilstm = SiameseClassifier.build_from_encoder(encoder_bilstm, max_len=max_len)\n",
        "\n",
        "encoder_cnn = SiameseEncoderFactory.create_cnn(vocab_size=vocab_size, max_len=max_len)\n",
        "model_cnn = SiameseClassifier.build_from_encoder(encoder_cnn, max_len=max_len)\n",
        "\n",
        "model_bilstm.summary()\n",
        "model_cnn.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=3, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "]\n",
        "\n",
        "metrics = [\n",
        "    keras.metrics.AUC(curve='ROC', name='auc'),\n",
        "    keras.metrics.AUC(curve='PR', name='auc_pr'),\n",
        "    keras.metrics.Precision(name='precision'),\n",
        "    keras.metrics.Recall(name='recall')\n",
        "]\n",
        "\n",
        "model_bilstm.compile(optimizer=keras.optimizers.Adam(LR), loss='binary_crossentropy', metrics=metrics)\n",
        "model_cnn.compile(optimizer=keras.optimizers.Adam(LR), loss='binary_crossentropy', metrics=metrics)\n",
        "\n",
        "history_bilstm = model_bilstm.fit(\n",
        "    [X_train_a, X_train_b], y_train,\n",
        "    validation_data=([X_val_a, X_val_b], y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_cnn = model_cnn.fit(\n",
        "    [X_train_a, X_train_b], y_train,\n",
        "    validation_data=([X_val_a, X_val_b], y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation utilities\n",
        "\n",
        "def evaluate_model(model: keras.Model, name: str,\n",
        "                   Xa: np.ndarray, Xb: np.ndarray, y_true: np.ndarray) -> Dict[str, Any]:\n",
        "    probs = model.predict([Xa, Xb], batch_size=1024, verbose=0).reshape(-1)\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_true, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, preds, average='binary', zero_division=0)\n",
        "    try:\n",
        "        roc = roc_auc_score(y_true, probs)\n",
        "    except Exception:\n",
        "        roc = float('nan')\n",
        "    try:\n",
        "        pr_auc = average_precision_score(y_true, probs)\n",
        "    except Exception:\n",
        "        pr_auc = float('nan')\n",
        "    cm = confusion_matrix(y_true, preds)\n",
        "    print(f\"[{name}] Acc={acc:.4f} P={precision:.4f} R={recall:.4f} F1={f1:.4f} ROC-AUC={roc:.4f} PR-AUC={pr_auc:.4f}\")\n",
        "    return {\n",
        "        'name': name,\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc,\n",
        "        'pr_auc': pr_auc,\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'probs': probs.tolist(),\n",
        "        'preds': preds.tolist()\n",
        "    }\n",
        "\n",
        "results = []\n",
        "results.append(evaluate_model(model_bilstm, 'BiLSTM+Attention', X_test_a, X_test_b, y_test))\n",
        "results.append(evaluate_model(model_cnn, 'CNN', X_test_a, X_test_b, y_test))\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "pd.DataFrame([{k:v for k,v in r.items() if k not in ('confusion_matrix','probs','preds')} for r in results])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plots: training curves and confusion matrices\n",
        "\n",
        "def plot_history(history: keras.callbacks.History, title: str, save_path: Optional[str] = None):\n",
        "    hist = history.history\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    axes[0].plot(hist['loss'], label='train')\n",
        "    axes[0].plot(hist['val_loss'], label='val')\n",
        "    axes[0].set_title(f'{title} - Loss')\n",
        "    axes[0].legend()\n",
        "\n",
        "    if 'auc' in hist and 'val_auc' in hist:\n",
        "        axes[1].plot(hist['auc'], label='train')\n",
        "        axes[1].plot(hist['val_auc'], label='val')\n",
        "        axes[1].set_title(f'{title} - ROC AUC')\n",
        "        axes[1].legend()\n",
        "    else:\n",
        "        axes[1].axis('off')\n",
        "\n",
        "    if 'auc_pr' in hist and 'val_auc_pr' in hist:\n",
        "        axes[2].plot(hist['auc_pr'], label='train')\n",
        "        axes[2].plot(hist['val_auc_pr'], label='val')\n",
        "        axes[2].set_title(f'{title} - PR AUC')\n",
        "        axes[2].legend()\n",
        "    else:\n",
        "        axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history_bilstm, 'BiLSTM+Attention', save_path='history_bilstm.png')\n",
        "plot_history(history_cnn, 'CNN', save_path='history_cnn.png')\n",
        "\n",
        "# Confusion matrices\n",
        "for r in results:\n",
        "    cm = np.array(r['confusion_matrix'])\n",
        "    fig = plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(f\"Confusion Matrix - {r['name']}\")\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    out = f\"cm_{r['name'].replace('+','_').replace(' ','_')}.png\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out, dpi=150)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qualitative examples: show some correct and incorrect predictions\n",
        "\n",
        "def sample_qualitative(pairs: List[PairRecord], probs: np.ndarray, preds: np.ndarray, y_true: np.ndarray,\n",
        "                       k: int = 5) -> Dict[str, List[Dict[str, Any]]]:\n",
        "    idxs = np.arange(len(pairs))\n",
        "    correct = idxs[preds == y_true]\n",
        "    incorrect = idxs[preds != y_true]\n",
        "    random.shuffle(correct)\n",
        "    random.shuffle(incorrect)\n",
        "    out = {\n",
        "        'correct': [],\n",
        "        'incorrect': []\n",
        "    }\n",
        "    for s, group in [(correct, 'correct'), (incorrect, 'incorrect')]:\n",
        "        for i in s[:k]:\n",
        "            p = pairs[i]\n",
        "            out[group].append({\n",
        "                'text_a': p.text_a,\n",
        "                'text_b': p.text_b,\n",
        "                'true_label': int(y_true[i]),\n",
        "                'pred_prob': float(probs[i]),\n",
        "                'pred_label': int(preds[i]),\n",
        "                'cat_a': p.cat_a,\n",
        "                'cat_b': p.cat_b\n",
        "            })\n",
        "    return out\n",
        "\n",
        "# Build predictions for both models\n",
        "probs_bilstm = np.array(results[0]['probs'])\n",
        "preds_bilstm = np.array(results[0]['preds'])\n",
        "probs_cnn = np.array(results[1]['probs'])\n",
        "preds_cnn = np.array(results[1]['preds'])\n",
        "\n",
        "qual_bilstm = sample_qualitative(test_pairs, probs_bilstm, preds_bilstm, y_test, k=6)\n",
        "qual_cnn = sample_qualitative(test_pairs, probs_cnn, preds_cnn, y_test, k=6)\n",
        "\n",
        "with open('qualitative_bilstm.json', 'w') as f:\n",
        "    json.dump(qual_bilstm, f, indent=2)\n",
        "with open('qualitative_cnn.json', 'w') as f:\n",
        "    json.dump(qual_cnn, f, indent=2)\n",
        "\n",
        "print('Examples (BiLSTM+Attention):')\n",
        "for ex in qual_bilstm['correct'][:3]:\n",
        "    print('\\n[CORRECT]')\n",
        "    print('A:', ex['text_a'][:200])\n",
        "    print('B:', ex['text_b'][:200])\n",
        "    print('True:', ex['true_label'], 'Pred:', ex['pred_label'], 'Prob:', f\"{ex['pred_prob']:.3f}\")\n",
        "for ex in qual_bilstm['incorrect'][:3]:\n",
        "    print('\\n[INCORRECT]')\n",
        "    print('A:', ex['text_a'][:200])\n",
        "    print('B:', ex['text_b'][:200])\n",
        "    print('True:', ex['true_label'], 'Pred:', ex['pred_label'], 'Prob:', f\"{ex['pred_prob']:.3f}\")\n",
        "\n",
        "print('\\nExamples (CNN):')\n",
        "for ex in qual_cnn['correct'][:3]:\n",
        "    print('\\n[CORRECT]')\n",
        "    print('A:', ex['text_a'][:200])\n",
        "    print('B:', ex['text_b'][:200])\n",
        "    print('True:', ex['true_label'], 'Pred:', ex['pred_label'], 'Prob:', f\"{ex['pred_prob']:.3f}\")\n",
        "for ex in qual_cnn['incorrect'][:3]:\n",
        "    print('\\n[INCORRECT]')\n",
        "    print('A:', ex['text_a'][:200])\n",
        "    print('B:', ex['text_b'][:200])\n",
        "    print('True:', ex['true_label'], 'Pred:', ex['pred_label'], 'Prob:', f\"{ex['pred_prob']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export artifacts for report (metrics table, figures, model timings)\n",
        "\n",
        "# Simple timing comparison captured from history objects (approximate per-epoch time)\n",
        "def estimate_epoch_times(history: keras.callbacks.History) -> float:\n",
        "    times = history.epoch\n",
        "    # Not directly available; as a proxy we compute total duration / epochs\n",
        "    return None\n",
        "\n",
        "# Save a compact CSV of key metrics\n",
        "summary_rows = []\n",
        "for r in results:\n",
        "    summary_rows.append({\n",
        "        'Model': r['name'],\n",
        "        'Accuracy': r['accuracy'],\n",
        "        'Precision': r['precision'],\n",
        "        'Recall': r['recall'],\n",
        "        'F1': r['f1'],\n",
        "        'ROC_AUC': r['roc_auc'],\n",
        "        'PR_AUC': r['pr_auc']\n",
        "    })\n",
        "metrics_df = pd.DataFrame(summary_rows)\n",
        "metrics_df.to_csv('metrics_summary.csv', index=False)\n",
        "metrics_df\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
